{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses, models, optimizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def data_scaling(data):             #스케일러, main\n",
    "    scaler = MinMaxScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def data_sliding(data, n_features, Time_window): #데이터 밀기, main\n",
    "    reshape_num = data.shape[0] - Time_window + 1\n",
    "    data = sliding_window_view(data, (Time_window, n_features))\n",
    "    data = data.reshape(reshape_num,Time_window, n_features,1)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def data_load_processing(FILENAME, Time_window=None): #데이터 로딩 및 스케일러 사용, main\n",
    "    df = pd.read_csv(FILENAME)\n",
    "    df = df.drop(['Time'], axis=1)\n",
    "    n_features = df.shape[1]\n",
    "    x_train, x_test = train_test_split(df, test_size=0.9, shuffle=False)\n",
    "    #x_validation, x_test = train_test_split(x_test, test_size=0.5, shuffle=False)\n",
    "\n",
    "    x_train = data_scaling(x_train)\n",
    "    x_test = data_scaling(x_test)\n",
    "    #x_validation = data_scaling(x_validation)\n",
    "\n",
    "    x_train = data_sliding(x_train, n_features, Time_window)\n",
    "    x_test = data_sliding(x_test, n_features, Time_window)\n",
    "    #x_validation = data_sliding(x_validation, n_features, Time_window)\n",
    "\n",
    "    return x_train, x_test #, x_validation\n",
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    tf.keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "\n",
    "def vae_model(input_shape, latent_dim, kernel_size, strides, padding):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    h = layers.Conv2D(32, kernel_size=kernel_size, strides=strides, padding=padding, activation='relu',\n",
    "                      name=\"conv1\", data_format=\"channels_first\")(inputs)\n",
    "    h = layers.Conv2D(64, kernel_size=kernel_size, strides=strides, padding=padding, activation='relu',\n",
    "                      name=\"conv2\", data_format=\"channels_first\")(h)\n",
    "    h = layers.Conv2D(128, kernel_size=kernel_size, strides=strides, padding=padding, activation='relu',\n",
    "                      name=\"conv3\", data_format=\"channels_first\")(h)\n",
    "    shape_save = h.get_shape().as_list()\n",
    "    h = layers.Flatten()(h)\n",
    "    h = layers.Dense(16, activation=\"relu\")(h)\n",
    "\n",
    "    # vae\n",
    "    z_mean = layers.Dense(latent_dim, name='z_mean')(h)\n",
    "    z_log_var = layers.Dense(latent_dim, name='z_log_var')(h)\n",
    "    z = sampling([z_mean, z_log_var])\n",
    "    encoder = tf.keras.Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "    # decoder\n",
    "    latent_inputs = tf.keras.Input(shape=(latent_dim,))\n",
    "    h = layers.Dense(shape_save[1] * shape_save[2] * shape_save[3], activation=\"relu\")(latent_inputs)\n",
    "    h = layers.Reshape((shape_save[1], shape_save[2], shape_save[3]))(h)\n",
    "    h = layers.Conv2DTranspose(64, kernel_size=kernel_size, strides=strides, padding=padding, activation='relu',\n",
    "                               data_format=\"channels_first\")(h)\n",
    "    h = layers.Conv2DTranspose(32, kernel_size=kernel_size, strides=strides, padding=padding, activation='relu',\n",
    "                               data_format=\"channels_first\")(h)\n",
    "    decoder_outputs = layers.Conv2DTranspose(1,kernel_size=kernel_size, strides=strides, padding=padding, activation='relu',\n",
    "                                             data_format=\"channels_first\")(h)\n",
    "    decoder = tf.keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "    return encoder, decoder\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "    #---------------------------------사용자 설정 변수---------------------------------------\n",
    "    FILENAME = \"BTC_USDT.csv\"\n",
    "    Time_window = 8\n",
    "    #-------------------------------------------------------------------------------------\n",
    "\n",
    "    #데이터 로딩\n",
    "    x_train, x_test= data_load_processing(FILENAME, Time_window=Time_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_model(input_shape, latent_dim, kernel_size, strides, padding):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    h = layers.Conv2D(32, kernel_size=(2,4), strides=(2,2), padding=\"same\", activation='relu',\n",
    "                      name=\"conv1\")(inputs)\n",
    "    h = layers.Conv2D(64, kernel_size=(2,4), strides=(2,2), padding=\"same\", activation='relu',\n",
    "                      name=\"conv2\")(h)\n",
    "    h = layers.Conv2D(128, kernel_size=(2,4), strides=(2,2), padding=\"same\", activation='relu',\n",
    "                      name=\"conv3\")(h)\n",
    "    shape_save = h.get_shape().as_list()\n",
    "    h = layers.Flatten()(h)\n",
    "    h = layers.Dense(32, activation=\"relu\")(h)\n",
    "\n",
    "    # vae\n",
    "    z_mean = layers.Dense(latent_dim, name='z_mean')(h)\n",
    "    z_log_var = layers.Dense(latent_dim, name='z_log_var')(h)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    encoder = tf.keras.Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "    # decoder\n",
    "    latent_inputs = tf.keras.Input(shape=(latent_dim,))\n",
    "    h = layers.Dense(shape_save[1] * shape_save[2] * shape_save[3], activation=\"relu\")(latent_inputs)\n",
    "    h = layers.Reshape((shape_save[1], shape_save[2], shape_save[3]))(h)\n",
    "    h = layers.Conv2DTranspose(64, kernel_size=(2,6), strides=(2,1), padding='valid', activation='relu')(h)\n",
    "    h = layers.Conv2DTranspose(32, kernel_size=(2,4), strides=(2,2), padding='valid', activation='relu')(h)\n",
    "    decoder_outputs = layers.Conv2DTranspose(1,kernel_size=(2,4), strides=(2,2), padding='valid', activation=\"relu\")(h)\n",
    "    decoder = tf.keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8, 54, 1)]   0           []                               \n",
      "                                                                                                  \n",
      " conv1 (Conv2D)                 (None, 4, 27, 32)    288         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2 (Conv2D)                 (None, 2, 14, 64)    16448       ['conv1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv3 (Conv2D)                 (None, 1, 7, 128)    65664       ['conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 896)          0           ['conv3[0][0]']                  \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           28704       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 3)            99          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 3)            99          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 3)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 111,302\n",
      "Trainable params: 111,302\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 896)               3584      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1, 7, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 2, 12, 64)        98368     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 4, 26, 32)        16416     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 8, 54, 1)         257       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118,625\n",
      "Trainable params: 118,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1233/1233 [==============================] - 27s 21ms/step - loss: 236.8166 - reconstruction_loss: 205.3145 - kl_loss: 2.8579\n",
      "Epoch 2/10\n",
      "1233/1233 [==============================] - 27s 22ms/step - loss: 197.3849 - reconstruction_loss: 194.4900 - kl_loss: 2.1190\n",
      "Epoch 3/10\n",
      "1233/1233 [==============================] - 27s 22ms/step - loss: 196.0878 - reconstruction_loss: 193.7132 - kl_loss: 1.9325\n",
      "Epoch 4/10\n",
      "1233/1233 [==============================] - 26s 21ms/step - loss: 194.9191 - reconstruction_loss: 193.2081 - kl_loss: 1.8098\n",
      "Epoch 5/10\n",
      "1233/1233 [==============================] - 26s 21ms/step - loss: 194.9608 - reconstruction_loss: 193.2296 - kl_loss: 1.7839\n",
      "Epoch 6/10\n",
      "1233/1233 [==============================] - 26s 21ms/step - loss: 194.9958 - reconstruction_loss: 193.1653 - kl_loss: 1.7749\n",
      "Epoch 7/10\n",
      "1233/1233 [==============================] - 26s 21ms/step - loss: 195.3264 - reconstruction_loss: 193.3599 - kl_loss: 1.7941\n",
      "Epoch 8/10\n",
      "1233/1233 [==============================] - 26s 21ms/step - loss: 194.8216 - reconstruction_loss: 192.8246 - kl_loss: 2.0338\n",
      "Epoch 9/10\n",
      "1233/1233 [==============================] - 26s 21ms/step - loss: 194.6303 - reconstruction_loss: 192.2561 - kl_loss: 2.2675\n",
      "Epoch 10/10\n",
      "1233/1233 [==============================] - 26s 21ms/step - loss: 194.1755 - reconstruction_loss: 191.9184 - kl_loss: 2.3548\n",
      "4932/4932 [==============================] - 10s 2ms/step\n",
      "4932/4932 [==============================] - 12s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "#VAE_model\n",
    "latent_dim = 3\n",
    "input_shape = (8, 54, 1)\n",
    "kernel_size = (2,8)\n",
    "strides = (2,2)\n",
    "padding = \"same\"\n",
    "\n",
    "encoder, decoder = vae_model(input_shape, latent_dim, kernel_size, strides, padding)\n",
    "encoder.summary()\n",
    "decoder.summary()\n",
    "\n",
    "#VAE\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "vae.fit(x_train, epochs=10, batch_size=128)\n",
    "\n",
    "z_mean_, z_log_var_, latent_z = vae.encoder.predict(x_train)\n",
    "\n",
    "decoded_latent_z = vae.decoder.predict(latent_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_preprocess(latent_z, seq_length):\n",
    "    reshape_num = latent_z.shape[0] - seq_length + 1\n",
    "    slide_z = sliding_window_view(latent_z, (seq_length, latent_z.shape[1]))\n",
    "    slide_z = slide_z.reshape(reshape_num, seq_length, latent_z.shape[1])\n",
    "    slide_z = slide_z[:-1]\n",
    "\n",
    "    dataY = []\n",
    "    for i in range(0, len(latent_z) - seq_length):\n",
    "        _y = latent_z[i + seq_length]  # 다음 나타날 z(정답)\n",
    "        dataY.append(_y)  # dataY 리스트에 추가\n",
    "    dataY = np.array(dataY)\n",
    "    \n",
    "    return slide_z, dataY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(seq_length, latent_dim, lstm_z, dataY):\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(seq_length, latent_dim))\n",
    "    h = layers.LSTM(128, activation=\"tanh\", name='lstm1')(inputs)\n",
    "    lstm_outputs = layers.Dense(latent_dim, activation='linear')(h)\n",
    "    lstm = tf.keras.Model(inputs, lstm_outputs, name='lstm')\n",
    "    lstm.summary()\n",
    "    lstm.compile(loss='mae', optimizer=tf.keras.optimizers.Adam())\n",
    "    lstm.fit(lstm_z, dataY, epochs=10, batch_size=518)\n",
    "    lstm_results = lstm.predict(lstm_z)\n",
    "    \n",
    "\n",
    "    return lstm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=30\n",
    "slide_z, dataY = lstm_preprocess(latent_z, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lstm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 30, 3)]           0         \n",
      "                                                                 \n",
      " lstm1 (LSTM)                (None, 128)               67584     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67,971\n",
      "Trainable params: 67,971\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "305/305 [==============================] - 37s 116ms/step - loss: 0.4957\n",
      "Epoch 2/10\n",
      "305/305 [==============================] - 35s 115ms/step - loss: 0.4858\n",
      "Epoch 3/10\n",
      "305/305 [==============================] - 35s 116ms/step - loss: 0.4855\n",
      "Epoch 4/10\n",
      "305/305 [==============================] - 35s 115ms/step - loss: 0.4853\n",
      "Epoch 5/10\n",
      "305/305 [==============================] - 35s 116ms/step - loss: 0.4852\n",
      "Epoch 6/10\n",
      "305/305 [==============================] - 35s 115ms/step - loss: 0.4850\n",
      "Epoch 7/10\n",
      "305/305 [==============================] - 36s 117ms/step - loss: 0.4850\n",
      "Epoch 8/10\n",
      "305/305 [==============================] - 37s 120ms/step - loss: 0.4846\n",
      "Epoch 9/10\n",
      "305/305 [==============================] - 36s 118ms/step - loss: 0.4845\n",
      "Epoch 10/10\n",
      "305/305 [==============================] - 35s 116ms/step - loss: 0.4843\n",
      "4931/4931 [==============================] - 60620s 12s/step\n"
     ]
    }
   ],
   "source": [
    "lstm_results=lstm_model(seq_length, latent_dim, slide_z, dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_, x_1 = train_test_split(df, test_size=0.9, shuffle=False)\n",
    "x_=x_[7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = pd.concat([pd.DataFrame(x_[\"Open\"][30:]).reset_index(drop=True),pd.DataFrame(lstm_results)],axis=1)\n",
    "result1.columns=[\"Price\",\"z0\",\"z1\",\"z2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1.to_csv(\"out_\"+FILENAME,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
