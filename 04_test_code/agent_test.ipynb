{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aea168c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = next(iter(range(3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f54d0a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(range(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2ac7946d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c141b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0503744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural import Net\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, state_size, is_eval=False, model_name=\"\"):\n",
    "        self.state_size = state_size  # normalized previous days\n",
    "        self.action_size = 3  # hold, buy, sell\n",
    "        self.memory = deque(maxlen=5000)\n",
    "        self.model_name = model_name\n",
    "        self.is_eval = is_eval\n",
    "\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.model = Net(self.state_size, self.action_size).float()\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            self.net = self.net.to(device='cuda')\n",
    "        if is_eval:\n",
    "            self.load(model_name)        \n",
    "        \n",
    "        self.model =  load if is_eval else \n",
    "        self.model_target = Net(self.state_size, self.action_size).float()\n",
    "        \n",
    "        self.model = load_model(model_name) if is_eval else self._model()\n",
    "        self.model_target = load_model(model_name) if is_eval else self._model()\n",
    "        self.update_target_from_model()\n",
    "        self.loss = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def update_target_from_model(self):\n",
    "        #Update the target model from the base model\n",
    "        self.model_target.set_weights(self.model.get_weights())\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        if not self.is_eval and np.random.rand() <= self.epsilon:\n",
    "            a1 = random.random()\n",
    "            a2 = random.uniform(0, 1 - a1)\n",
    "            a3 = 1 - a1 - a2\n",
    "            return np.array([[a1, a2, a3]])\n",
    "\n",
    "        options = self.model.predict(state)\n",
    "        return softmax(options)\n",
    "\n",
    "    def exp_replay(self, batch_size):\n",
    "\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        states = np.array([tup[0][0] for tup in minibatch])\n",
    "        actions = np.array([tup[1] for tup in minibatch])\n",
    "        rewards = np.array([tup[2] for tup in minibatch])\n",
    "        next_states = np.array([tup[3][0] for tup in minibatch])\n",
    "        done = np.array([tup[4] for tup in minibatch])\n",
    "\n",
    "        st_predict = self.model.predict(states)\n",
    "        nst_predict = self.model.predict(next_states)\n",
    "        nst_predict_target = self.model_target.predict(next_states)\n",
    "\n",
    "        nst_predict_max_index = np.argmax(nst_predict, axis=1) # leanring agent의 Q값중 큰 action\n",
    "        one_hot_max_index = tf.one_hot(nst_predict_max_index, self.action_size)\n",
    "\n",
    "        target = rewards + self.gamma * np.amax(nst_predict_target*one_hot_max_index,axis=1) # 미래\n",
    "        target[done] = rewards[done]\n",
    "\n",
    "        target_f = st_predict\n",
    "        target_f[range(batch_size), actions] = target\n",
    "\n",
    "        # Q(s', a)\n",
    "        #target = rewards + self.gamma * np.amax(self.model.predict(next_states), axis=1) #미래\n",
    "        # end state target is reward itself (no lookahead)\n",
    "        #target[done] = rewards[done]\n",
    "\n",
    "        # Q(s, a)\n",
    "        #target_f = self.model.predict(states) #현재로 예측하고? 사실상 array 만들어주는 역할, q(s,a)\n",
    "\n",
    "        # make the agent to approximately map the current state to future discounted reward\n",
    "        #target_f[range(batch_size), actions] = target #Q(s', a) 값을 업데이트, argmaxQ(s_t+1,a)\n",
    "\n",
    "        hist = self.model.fit(states, target_f, epochs=1, verbose=0) #현재 스테이트 넣고 계산된 미래 Q값을 학습시키는 것\n",
    "        #print(hist.history['loss'])\n",
    "        #self.loss.append(hist.history['loss'][0]\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8dc029",
   "metadata": {},
   "source": [
    "## nerual.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372a21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import copy\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim,300)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300,200)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200,100)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, output_dim)\n",
    "        )\n",
    "\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == 'online':\n",
    "            return self.online(input)\n",
    "        elif model == 'target':\n",
    "            return self.target(input)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5eeb3e",
   "metadata": {},
   "source": [
    "## 통째로 바꿔보기 agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f07f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from neural import Net\n",
    "from collections import deque\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, state_size, save_dir, checkpoint=None):\n",
    "        self.state_dim = state_size\n",
    "        self.action_dim = 3\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = 32\n",
    "\n",
    "        self.exploration_rate = 1                  #epsilon\n",
    "        self.exploration_rate_decay = 0.9995       #epsilon_dacay\n",
    "        self.exploration_rate_min = 0.1            #epsilon_min\n",
    "        self.gamma = 0.95\n",
    "\n",
    "        self.curr_step = 0\n",
    "        self.burnin = 100  # min. experiences before training\n",
    "        self.learn_every = 3   # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1000   # no. of experiences between Q_target & Q_online sync <???>\n",
    "\n",
    "        self.save_every = 500   # no. of experiences between saving Agent\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "        self.net = Net(self.state_dim, self.action_dim).float()\n",
    "        if self.use_cuda: #쿠다 사용여부\n",
    "            self.net = self.net.to(device='cuda')\n",
    "        if checkpoint:\n",
    "            self.load(checkpoint)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.MSELoss() #MSE로 로스 펑션 사용, RMSE도 고려해볼만하다.\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        # EXPLORE\n",
    "        #if np.random.rand() < self.exploration_rate:\n",
    "        #    action_idx = np.random.randint(self.action_dim)\n",
    "            \n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            a1 = random.random()\n",
    "            a2 = random.uniform(0, 1 - a1)\n",
    "            a3 = 1 - a1 - a2\n",
    "            action_values = torch.from_numpy(np.array([[a1, a2, a3]]))    \n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
    "            #state = state.unsqueeze(0)\n",
    "            action_values = F.softmax(self.net(state, model='online'),dim=0) #모델을 거쳤을 때 각 행동의 기대값 ex) 3가지 액션일때 [[0.1, 0.3, 0.6]]\n",
    "           \n",
    "        action_idx = torch.argmax(action_values, axis=1).item()\n",
    "        \n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return torch.max(action_values), action_idx\n",
    "        \n",
    "    \n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "        Inputs:\n",
    "        state (LazyFrame),\n",
    "        next_state (LazyFrame),\n",
    "        action (int),\n",
    "        reward (float),\n",
    "        done(bool))\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
    "        next_state = torch.FloatTensor(next_state).cuda() if self.use_cuda else torch.FloatTensor(next_state)\n",
    "        action = torch.LongTensor([action]).cuda() if self.use_cuda else torch.LongTensor([action])\n",
    "        reward = torch.DoubleTensor([reward]).cuda() if self.use_cuda else torch.DoubleTensor([reward])\n",
    "        done = torch.BoolTensor([done]).cuda() if self.use_cuda else torch.BoolTensor([done])\n",
    "\n",
    "        self.memory.append( (state, next_state, action, reward, done,) )\n",
    "\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model='online')[np.arange(0, self.batch_size), action] # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state)\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model='target')[np.arange(0, self.batch_size), best_action]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
    "\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target) :\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "        save_path = self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        torch.save(\n",
    "            dict(\n",
    "                model=self.net.state_dict(),\n",
    "                exploration_rate=self.exploration_rate\n",
    "            ),\n",
    "            save_path\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n",
    "\n",
    "\n",
    "    def load(self, load_path):\n",
    "        if not load_path.exists():\n",
    "            raise ValueError(f\"{load_path} does not exist\")\n",
    "\n",
    "        ckp = torch.load(load_path, map_location=('cuda' if self.use_cuda else 'cpu'))\n",
    "        exploration_rate = ckp.get('exploration_rate')\n",
    "        state_dict = ckp.get('model')\n",
    "\n",
    "        print(f\"Loading model at {load_path} with exploration rate {exploration_rate}\")\n",
    "        self.net.load_state_dict(state_dict)\n",
    "        self.exploration_rate = exploration_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e8f844",
   "metadata": {},
   "source": [
    "## metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683af900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MetricLogger():\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "            plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5ee4a7",
   "metadata": {},
   "source": [
    "## Main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85256d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import random, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from metrics import MetricLogger\n",
    "from agent import Agent\n",
    "from Dataset import CoinDataset\n",
    "from env import env\n",
    "\n",
    "# Initialize Super Mario environment\n",
    "trade_env =  env(learning_length=2400,\"out_data.csv\")#데이터 불러오기\n",
    "\n",
    "\n",
    "save_dir = Path('checkpoints') / datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "checkpoint = None # Path('checkpoints/2020-10-21T18-25-27/mario.chkpt')\n",
    "\n",
    "state_size = trade_env.state_data.shape[1]\n",
    "    \n",
    "trade_agent = Agent(state_dim=state_size, action_dim=3, save_dir=save_dir, checkpoint=checkpoint)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 40000\n",
    "\n",
    "### for Loop that train the model num_episodes times by playing the game\n",
    "for e in range(episodes):\n",
    "    \n",
    "    #state = env.reset()\n",
    "    state = trade_env.state_data[0]\n",
    "    L_num = 100\n",
    "    prev_num = 0\n",
    "    now_length = 0\n",
    "    \n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # 3. Show environment (the visual) [WIP]\n",
    "        # env.render()\n",
    "\n",
    "        # 4. Run agent on the state\n",
    "        q_value, action = trade_agent.act(state)\n",
    "        \n",
    "        \n",
    "        # 5. Agent performs action\n",
    "        next_state, reward, done, now_length = trade_env.step(action, q_value, L_num, prev_num, now_length, done)\n",
    "\n",
    "        # 6. Remember\n",
    "        trade_agent.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # 7. Learn\n",
    "        q, loss = trade_agent.learn()\n",
    "\n",
    "        # 8. Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # 9. Update state\n",
    "        state = next_state\n",
    "\n",
    "        # 10. Check if end of game\n",
    "        if done :\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        logger.record(\n",
    "            episode=e,\n",
    "            epsilon=mario.exploration_rate,\n",
    "            step=mario.curr_step\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b80b6d6",
   "metadata": {},
   "source": [
    "## Dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0dde64a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CoinDataset(Dataset):\n",
    "    def __init__(self, file_name):\n",
    "        self.raw_data = pd.read_csv(file_name)\n",
    "        self.state_data = torch.from_numpy(raw_data.iloc[:,1:].values)\n",
    "        self.price_data = torch.from_numpy(raw_data[\"Price\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.state_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.state_data[idx].unsqueeze(0), self.price_data[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c39131b",
   "metadata": {},
   "source": [
    "## env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a3003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from Dataset import CoinDataset\n",
    "\n",
    "class env():\n",
    "    def __init__(self, learning_length, file_name):\n",
    "        self.dataset = torch.utils.data.DataLoader(CoinDataset(file_name), batch_size=learning_length)\n",
    "        self.state_data = self.dataset.state_data\n",
    "        self.price_data = self.dataset.price_data\n",
    "        \n",
    "    def step(self, action,q_value, L_num, prev_num, now_length, done):\n",
    "        reward = 0.0\n",
    "        price_t0 = self.price_data[now_length]\n",
    "        now_length += 1\n",
    "        price_t1 = self.price_data[now_length]\n",
    "        price_diff = price_t1-price_t0\n",
    "        \n",
    "        num=q_value*L_num\n",
    "        num_diff = num - prev_num\n",
    "        reward = num_diff * price_diff\n",
    "        \n",
    "        next_state = self.state_data[now_length]\n",
    "        \n",
    "        if now_length == learning_length :\n",
    "            done = True\n",
    "            \n",
    "        return next_state, reward, done, now_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba6ccf",
   "metadata": {},
   "source": [
    "## replay.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded35d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, TransformObservation\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "from metrics import MetricLogger\n",
    "from agent import Mario\n",
    "from wrappers import ResizeObservation, SkipFrame\n",
    "\n",
    "# Initialize Super Mario environment\n",
    "trade_env = env(learning_length=2502, file_name=\"out_data.csv\")  # 데이터 불러오기\n",
    "\n",
    "save_dir = Path('checkpoints') / datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "checkpoint = None  # Path('checkpoints/2020-10-21T18-25-27/mario.chkpt')\n",
    "\n",
    "state_data, price_data = next(iter(trade_env.data))\n",
    "\n",
    "trade_agent = Agent(state_size=3, save_dir=save_dir, checkpoint=checkpoint)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 2000\n",
    "trade_agnet.exploration_rate = trade_agnet.exploration_rate_min\n",
    "\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "\n",
    "\n",
    "        action = mario.act(state)\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        logger.log_step(reward, None, None)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done or info['flag_get']:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        logger.record(\n",
    "            episode=e,\n",
    "            epsilon=mario.exploration_rate,\n",
    "            step=mario.curr_step\n",
    "        )\n",
    "        \n",
    "for e in range(episodes):\n",
    "\n",
    "    state = trade_env.reset()\n",
    "    L_num = 100\n",
    "    prev_num = 0\n",
    "    trade_agent.curr_step = 0\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "\n",
    "        # 4. Run agent on the state\n",
    "        q_value, action = trade_agent.act(state) #여기서 curr_step이 1올라감\n",
    "\n",
    "        # 5. Agent performs action\n",
    "        next_state, reward, total_profit, done = trade_env.step(action, q_value, L_num, prev_num, trade_agent.curr_step)\n",
    "\n",
    "        # 6. Remember\n",
    "        trade_agent.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # 8. Logging\n",
    "        logger.log_step(reward.detach().cpu().numpy(), None, None)\n",
    "\n",
    "        # 9. Update state\n",
    "        state = next_state\n",
    "\n",
    "        # 10. Check if end of game\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        logger.record(\n",
    "            episode=e,\n",
    "            epsilon=trade_agent.exploration_rate,\n",
    "            step=trade_agent.curr_step\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0052229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "import random, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from metrics import MetricLogger\n",
    "from agent import Agent\n",
    "from env import env\n",
    "\n",
    "# Initialize Super Mario environment\n",
    "trade_env = env(learning_length=2502, file_name=\"out_data.csv\")  # 데이터 불러오기\n",
    "\n",
    "save_dir = Path('checkpoints') / datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "checkpoint = None  # Path('checkpoints/2020-10-21T18-25-27/mario.chkpt')\n",
    "\n",
    "state_size, _ = trade_env.coindata[1] #하나 부를땐 이거\n",
    "state_data, price_data = next(iter(trade_env.data)) #데이터 로더에 넣어서 부를땐??\n",
    "\n",
    "trade_agent = Agent(state_size=3, save_dir=save_dir, checkpoint=checkpoint)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 2000\n",
    "\n",
    "### for Loop that train the model num_episodes times by playing the game\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = trade_env.reset()\n",
    "    L_num = 100\n",
    "    prev_num = 0\n",
    "    trade_agent.curr_step = 0\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # 3. Show environment (the visual) [WIP]\n",
    "        # env.render()\n",
    "\n",
    "        # 4. Run agent on the state\n",
    "        q_value, action = trade_agent.act(state) #여기서 curr_step이 1올라감\n",
    "\n",
    "        # 5. Agent performs action\n",
    "        next_state, reward, total_profit, done = trade_env.step(action, q_value, L_num, prev_num, trade_agent.curr_step)\n",
    "\n",
    "        # 6. Remember\n",
    "        trade_agent.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # 7. Learn\n",
    "        q, loss = trade_agent.learn()\n",
    "\n",
    "        # 8. Logging\n",
    "        logger.log_step(reward.detach().cpu().numpy(), loss, q)\n",
    "\n",
    "        # 9. Update state\n",
    "        state = next_state\n",
    "\n",
    "        # 10. Check if end of game\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        logger.record(\n",
    "            episode=e,\n",
    "            epsilon=trade_agent.exploration_rate,\n",
    "            step=trade_agent.curr_step\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d509382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "a = np.random.rand(10,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c0d46fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.DataFrame(a)\n",
    "k=a.iloc[1:,1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3233fdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e3bdd0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.168702</td>\n",
       "      <td>0.444208</td>\n",
       "      <td>0.340190</td>\n",
       "      <td>0.408997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.199491</td>\n",
       "      <td>0.547270</td>\n",
       "      <td>0.777295</td>\n",
       "      <td>0.443051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.944127</td>\n",
       "      <td>0.615175</td>\n",
       "      <td>0.782584</td>\n",
       "      <td>0.703338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.453528</td>\n",
       "      <td>0.540171</td>\n",
       "      <td>0.556252</td>\n",
       "      <td>0.541182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.843560</td>\n",
       "      <td>0.007810</td>\n",
       "      <td>0.888476</td>\n",
       "      <td>0.183707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.361536</td>\n",
       "      <td>0.967983</td>\n",
       "      <td>0.557054</td>\n",
       "      <td>0.390935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.713508</td>\n",
       "      <td>0.142903</td>\n",
       "      <td>0.649832</td>\n",
       "      <td>0.250312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.616661</td>\n",
       "      <td>0.138813</td>\n",
       "      <td>0.634591</td>\n",
       "      <td>0.042524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.193803</td>\n",
       "      <td>0.233025</td>\n",
       "      <td>0.184401</td>\n",
       "      <td>0.003505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.959506</td>\n",
       "      <td>0.508941</td>\n",
       "      <td>0.933035</td>\n",
       "      <td>0.354339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3\n",
       "0  0.168702  0.444208  0.340190  0.408997\n",
       "1  0.199491  0.547270  0.777295  0.443051\n",
       "2  0.944127  0.615175  0.782584  0.703338\n",
       "3  0.453528  0.540171  0.556252  0.541182\n",
       "4  0.843560  0.007810  0.888476  0.183707\n",
       "5  0.361536  0.967983  0.557054  0.390935\n",
       "6  0.713508  0.142903  0.649832  0.250312\n",
       "7  0.616661  0.138813  0.634591  0.042524\n",
       "8  0.193803  0.233025  0.184401  0.003505\n",
       "9  0.959506  0.508941  0.933035  0.354339"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "de95365d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54727008, 0.77729497, 0.44305057],\n",
       "       [0.61517515, 0.78258445, 0.70333836],\n",
       "       [0.54017053, 0.55625197, 0.54118164],\n",
       "       [0.00780967, 0.88847635, 0.18370694],\n",
       "       [0.96798262, 0.55705403, 0.39093511],\n",
       "       [0.14290295, 0.64983176, 0.25031229],\n",
       "       [0.13881277, 0.6345908 , 0.04252441],\n",
       "       [0.23302539, 0.18440119, 0.00350493],\n",
       "       [0.50894118, 0.93303489, 0.3543395 ]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "773fcd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 3])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(k).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dcf821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=torch.from_numpy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2d9d4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0595, 0.9188, 0.1970, 0.1810, 0.7091, 0.7207, 0.1504, 0.2708, 0.1461,\n",
       "         0.0186, 0.1998, 0.9948, 0.4809, 0.7254, 0.5702, 0.8193, 0.1620, 0.5140,\n",
       "         0.0561, 0.7193, 0.5418, 0.6215, 0.0038, 0.8768, 0.1731, 0.6158, 0.8829,\n",
       "         0.7870, 0.6645, 0.0460]], dtype=torch.float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "213d9378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "916775bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.297115  , 0.50277847, 0.515187  ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09f748de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.297115  , 0.50277847, 0.515187  ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1b27d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.297115  , 0.50277847, 0.515187  , 0.20275523, 0.5190563 ,\n",
       "        0.39580412, 0.01170525, 0.33156668, 0.38176398, 0.08626995,\n",
       "        0.97852628, 0.94416906, 0.31826684, 0.06503326, 0.39888082,\n",
       "        0.3815041 , 0.58378501, 0.72640227, 0.59115665, 0.87793077,\n",
       "        0.12857936, 0.06005596, 0.12663981, 0.16042522, 0.65099473,\n",
       "        0.01920423, 0.19659542, 0.70084941, 0.67971793, 0.63033119]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e981cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        self.total_profit = torch.FloatTensor(0).to(self.device)\n",
    "        self.reward = torch.FloatTensor(0).to(self.device)\n",
    "        self.price_diff = torch.FloatTensor(0).to(self.device)\n",
    "        self.num_diff = torch.FloatTensor(0).to(self.device)\n",
    "        self.num = torch.FloatTensor(0).to(self.device)\n",
    "        self.action = torch.FloatTensor(0).to(self.device)\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.total_profit = 0\n",
    "\n",
    "        return self.state_data[0]\n",
    "\n",
    "    #@torch.no_grad()\n",
    "    def step(self, action, q_value, L_num, prev_num, curr_step):\n",
    "        \n",
    "        self.action = torch.FloatTensor(action).to(self.device)\n",
    "        done = torch.bool(False).to(self.device)\n",
    "        #price_t0 = self.price_data[curr_step-1]\n",
    "        #price_t1 = self.price_data[curr_step]\n",
    "\n",
    "        self.price_diff = self.price_data[curr_step] - self.price_data[curr_step-1]\n",
    "\n",
    "        #action 에따라 -,+ 바꿔줘야함 action[0,1,2]\n",
    "        if self.action ==  torch.FloatTensor(1).to(self.device):\n",
    "            self.num =  torch.FloatTensor(0).to(self.device)\n",
    "\n",
    "        else:\n",
    "            if self.action ==  torch.FloatTensor(0).to(self.device): #매도\n",
    "                self.action =  torch.FloatTensor(-1).to(self.device)\n",
    "\n",
    "            if self.action ==  torch.FloatTensor(2).to(self.device): #매수\n",
    "                self.action =  torch.FloatTensor(1).to(self.device)\n",
    "\n",
    "            self.num = action * q_value * L_num\n",
    "\n",
    "        self.num_diff = prev_num + self.num\n",
    "        self.reward = self.num_diff * self.price_diff\n",
    "        self.total_profit += self.reward\n",
    "        next_state = self.state_data[curr_step+1]\n",
    "\n",
    "\n",
    "        if curr_step == self.learning_length-2:\n",
    "            done = True\n",
    "\n",
    "        return next_state, self.reward, self.total_profit, self.num, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047b88b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "self.total_profit = torch.FloatTensor(0).to(self.device)\n",
    "self.reward = torch.FloatTensor(0).to(self.device)\n",
    "self.price_diff = torch.FloatTensor(0).to(self.device)\n",
    "self.num_diff = torch.FloatTensor(0).to(self.device)\n",
    "self.num = torch.FloatTensor(0).to(self.device)\n",
    "self.action = torch.FloatTensor(0).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6440badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb95768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "865a5890",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = torch.FloatTensor([1]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f0f1fd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce23a503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "if reward == 1:\n",
    "    print(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c14afcf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98f9777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(range(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df3d4c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    k=next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9f27a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더\n",
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
    "\n",
    "dataset = TensorDataset(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ee3f6141",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee561037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[73., 80., 75.],\n",
       "         [93., 88., 93.]]),\n",
       " tensor([[152.],\n",
       "         [185.]])]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cec56225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[73., 80., 75.],\n",
      "        [93., 88., 93.]]) tensor([[152.],\n",
      "        [185.]])\n",
      "tensor([[ 89.,  91.,  90.],\n",
      "        [ 96.,  98., 100.]]) tensor([[180.],\n",
      "        [196.]])\n",
      "tensor([[73., 66., 70.]]) tensor([[142.]])\n",
      "tensor([[73., 80., 75.],\n",
      "        [93., 88., 93.]]) tensor([[152.],\n",
      "        [185.]])\n",
      "tensor([[ 89.,  91.,  90.],\n",
      "        [ 96.,  98., 100.]]) tensor([[180.],\n",
      "        [196.]])\n",
      "tensor([[73., 66., 70.]]) tensor([[142.]])\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    for a,b in dataloader:\n",
    "        print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "32fa9ad3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[73., 80., 75.],\n",
       "         [93., 88., 93.]]),\n",
       " tensor([[152.],\n",
       "         [185.]]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd32eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "99e3c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, samples in enumerate(dataloader):\n",
    "    if batch_idx == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ece7ca23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "126073ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[73., 80., 75.],\n",
       "         [93., 88., 93.]]),\n",
       " tensor([[152.],\n",
       "         [185.]])]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0a934d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
