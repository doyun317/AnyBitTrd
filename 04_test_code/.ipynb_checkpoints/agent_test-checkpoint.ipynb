{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c42a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca6cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural import Net\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, state_size, is_eval=False, model_name=\"\"):\n",
    "        self.state_size = state_size  # normalized previous days\n",
    "        self.action_size = 3  # hold, buy, sell\n",
    "        self.memory = deque(maxlen=5000)\n",
    "        self.model_name = model_name\n",
    "        self.is_eval = is_eval\n",
    "\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.model = Net(self.state_size, self.action_size).float()\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            self.net = self.net.to(device='cuda')\n",
    "        if is_eval:\n",
    "            self.load(model_name)        \n",
    "        \n",
    "        self.model =  load if is_eval else \n",
    "        self.model_target = Net(self.state_size, self.action_size).float()\n",
    "        \n",
    "        self.model = load_model(model_name) if is_eval else self._model()\n",
    "        self.model_target = load_model(model_name) if is_eval else self._model()\n",
    "        self.update_target_from_model()\n",
    "        self.loss = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def update_target_from_model(self):\n",
    "        #Update the target model from the base model\n",
    "        self.model_target.set_weights(self.model.get_weights())\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        if not self.is_eval and np.random.rand() <= self.epsilon:\n",
    "            a1 = random.random()\n",
    "            a2 = random.uniform(0, 1 - a1)\n",
    "            a3 = 1 - a1 - a2\n",
    "            return np.array([[a1, a2, a3]])\n",
    "\n",
    "        options = self.model.predict(state)\n",
    "        return softmax(options)\n",
    "\n",
    "    def exp_replay(self, batch_size):\n",
    "\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        states = np.array([tup[0][0] for tup in minibatch])\n",
    "        actions = np.array([tup[1] for tup in minibatch])\n",
    "        rewards = np.array([tup[2] for tup in minibatch])\n",
    "        next_states = np.array([tup[3][0] for tup in minibatch])\n",
    "        done = np.array([tup[4] for tup in minibatch])\n",
    "\n",
    "        st_predict = self.model.predict(states)\n",
    "        nst_predict = self.model.predict(next_states)\n",
    "        nst_predict_target = self.model_target.predict(next_states)\n",
    "\n",
    "        nst_predict_max_index = np.argmax(nst_predict, axis=1) # leanring agent의 Q값중 큰 action\n",
    "        one_hot_max_index = tf.one_hot(nst_predict_max_index, self.action_size)\n",
    "\n",
    "        target = rewards + self.gamma * np.amax(nst_predict_target*one_hot_max_index,axis=1) # 미래\n",
    "        target[done] = rewards[done]\n",
    "\n",
    "        target_f = st_predict\n",
    "        target_f[range(batch_size), actions] = target\n",
    "\n",
    "        # Q(s', a)\n",
    "        #target = rewards + self.gamma * np.amax(self.model.predict(next_states), axis=1) #미래\n",
    "        # end state target is reward itself (no lookahead)\n",
    "        #target[done] = rewards[done]\n",
    "\n",
    "        # Q(s, a)\n",
    "        #target_f = self.model.predict(states) #현재로 예측하고? 사실상 array 만들어주는 역할, q(s,a)\n",
    "\n",
    "        # make the agent to approximately map the current state to future discounted reward\n",
    "        #target_f[range(batch_size), actions] = target #Q(s', a) 값을 업데이트, argmaxQ(s_t+1,a)\n",
    "\n",
    "        hist = self.model.fit(states, target_f, epochs=1, verbose=0) #현재 스테이트 넣고 계산된 미래 Q값을 학습시키는 것\n",
    "        #print(hist.history['loss'])\n",
    "        #self.loss.append(hist.history['loss'][0]\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac5341",
   "metadata": {},
   "source": [
    "## nerual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd5cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim,300)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300,200)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200,100)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa7276",
   "metadata": {},
   "source": [
    "## 통째로 바꿔보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b06025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from neural import Net\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, state_size, save_dir, checkpoint=None):\n",
    "        self.state_dim = state_size\n",
    "        self.action_dim = 3\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = 32\n",
    "\n",
    "        self.exploration_rate = 1                  #epsilon\n",
    "        self.exploration_rate_decay = 0.9995       #epsilon_dacay\n",
    "        self.exploration_rate_min = 0.1            #epsilon_min\n",
    "        self.gamma = 0.95\n",
    "\n",
    "        self.curr_step = 0\n",
    "        self.burnin = 100  # min. experiences before training\n",
    "        self.learn_every = 3   # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1000   # no. of experiences between Q_target & Q_online sync <???>\n",
    "\n",
    "        self.save_every = 500   # no. of experiences between saving Agent\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "        self.net = Net(self.state_dim, self.action_dim).float()\n",
    "        if self.use_cuda:\n",
    "            self.net = self.net.to(device='cuda')\n",
    "        if checkpoint:\n",
    "            self.load(checkpoint)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        # EXPLORE\n",
    "        #if np.random.rand() < self.exploration_rate:\n",
    "        #    action_idx = np.random.randint(self.action_dim)\n",
    "            \n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            a1 = random.random()\n",
    "            a2 = random.uniform(0, 1 - a1)\n",
    "            a3 = 1 - a1 - a2\n",
    "            action_values = np.array([[a1, a2, a3]])    \n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
    "            #state = state.unsqueeze(0)\n",
    "            action_values = self.net(state)\n",
    "           \n",
    "        action_idx = torch.argmax(action_values, axis=1).item()\n",
    "        \n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx\n",
    "        \n",
    "    \n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "        Inputs:\n",
    "        state (LazyFrame),\n",
    "        next_state (LazyFrame),\n",
    "        action (int),\n",
    "        reward (float),\n",
    "        done(bool))\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
    "        next_state = torch.FloatTensor(next_state).cuda() if self.use_cuda else torch.FloatTensor(next_state)\n",
    "        action = torch.LongTensor([action]).cuda() if self.use_cuda else torch.LongTensor([action])\n",
    "        reward = torch.DoubleTensor([reward]).cuda() if self.use_cuda else torch.DoubleTensor([reward])\n",
    "        done = torch.BoolTensor([done]).cuda() if self.use_cuda else torch.BoolTensor([done])\n",
    "\n",
    "        self.memory.append( (state, next_state, action, reward, done,) )\n",
    "\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state)[np.arange(0, self.batch_size), action] # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state)\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model='target')[np.arange(0, self.batch_size), best_action]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
    "\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target) :\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "        save_path = self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        torch.save(\n",
    "            dict(\n",
    "                model=self.net.state_dict(),\n",
    "                exploration_rate=self.exploration_rate\n",
    "            ),\n",
    "            save_path\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n",
    "\n",
    "\n",
    "    def load(self, load_path):\n",
    "        if not load_path.exists():\n",
    "            raise ValueError(f\"{load_path} does not exist\")\n",
    "\n",
    "        ckp = torch.load(load_path, map_location=('cuda' if self.use_cuda else 'cpu'))\n",
    "        exploration_rate = ckp.get('exploration_rate')\n",
    "        state_dict = ckp.get('model')\n",
    "\n",
    "        print(f\"Loading model at {load_path} with exploration rate {exploration_rate}\")\n",
    "        self.net.load_state_dict(state_dict)\n",
    "        self.exploration_rate = exploration_rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
