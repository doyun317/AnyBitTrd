{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "import torch as th\n",
    "\n",
    "model = A2C(\"MlpPolicy\", \"CartPole-v1\").learn(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 27.3     |\n",
      "|    ep_rew_mean        | 27.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 858      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.657   |\n",
      "|    explained_variance | 0.186    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -2.42    |\n",
      "|    value_loss         | 18.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 36.5     |\n",
      "|    ep_rew_mean        | 36.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 891      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.628   |\n",
      "|    explained_variance | -0.0576  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 1.77     |\n",
      "|    value_loss         | 7.57     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 41.7     |\n",
      "|    ep_rew_mean        | 41.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 897      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.507   |\n",
      "|    explained_variance | -0.0164  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 2.09     |\n",
      "|    value_loss         | 6.44     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 49.5     |\n",
      "|    ep_rew_mean        | 49.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 909      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.601   |\n",
      "|    explained_variance | -0.00167 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 1.47     |\n",
      "|    value_loss         | 5.56     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 52.6     |\n",
      "|    ep_rew_mean        | 52.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 896      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.579   |\n",
      "|    explained_variance | 0.0172   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 1.21     |\n",
      "|    value_loss         | 5.01     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 53.5     |\n",
      "|    ep_rew_mean        | 53.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 893      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.555   |\n",
      "|    explained_variance | 0.000451 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 1.2      |\n",
      "|    value_loss         | 4.41     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 56.2     |\n",
      "|    ep_rew_mean        | 56.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 892      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.556   |\n",
      "|    explained_variance | 0.00675  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 1.35     |\n",
      "|    value_loss         | 3.85     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 59.7     |\n",
      "|    ep_rew_mean        | 59.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 890      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.516   |\n",
      "|    explained_variance | 0.00104  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.499    |\n",
      "|    value_loss         | 3.35     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 62.6      |\n",
      "|    ep_rew_mean        | 62.6      |\n",
      "| time/                 |           |\n",
      "|    fps                | 882       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.543    |\n",
      "|    explained_variance | -0.000336 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 0.662     |\n",
      "|    value_loss         | 2.88      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 62.5      |\n",
      "|    ep_rew_mean        | 62.5      |\n",
      "| time/                 |           |\n",
      "|    fps                | 882       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.607    |\n",
      "|    explained_variance | -0.000194 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 0.838     |\n",
      "|    value_loss         | 2.47      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 64.7     |\n",
      "|    ep_rew_mean        | 64.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 878      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.542   |\n",
      "|    explained_variance | 9.51e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.799    |\n",
      "|    value_loss         | 2.07     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 67.5     |\n",
      "|    ep_rew_mean        | 67.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 880      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.394   |\n",
      "|    explained_variance | 0.000494 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.899    |\n",
      "|    value_loss         | 1.69     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 71.7     |\n",
      "|    ep_rew_mean        | 71.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 883      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.502   |\n",
      "|    explained_variance | 0.00015  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.57     |\n",
      "|    value_loss         | 1.34     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 74.7     |\n",
      "|    ep_rew_mean        | 74.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 871      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.527   |\n",
      "|    explained_variance | 4.54e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.259    |\n",
      "|    value_loss         | 1.04     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 76.6     |\n",
      "|    ep_rew_mean        | 76.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 874      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.555   |\n",
      "|    explained_variance | 9.29e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.371    |\n",
      "|    value_loss         | 0.773    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 79.2     |\n",
      "|    ep_rew_mean        | 79.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 876      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.525   |\n",
      "|    explained_variance | 3.49e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.249    |\n",
      "|    value_loss         | 0.542    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 84.5      |\n",
      "|    ep_rew_mean        | 84.5      |\n",
      "| time/                 |           |\n",
      "|    fps                | 877       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.509    |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 0.178     |\n",
      "|    value_loss         | 0.355     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 88.2      |\n",
      "|    ep_rew_mean        | 88.2      |\n",
      "| time/                 |           |\n",
      "|    fps                | 872       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.511    |\n",
      "|    explained_variance | -2.25e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 0.107     |\n",
      "|    value_loss         | 0.205     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 92.5     |\n",
      "|    ep_rew_mean        | 92.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 871      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.44    |\n",
      "|    explained_variance | 8.16e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.143    |\n",
      "|    value_loss         | 0.0974   |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 96.5      |\n",
      "|    ep_rew_mean        | 96.5      |\n",
      "| time/                 |           |\n",
      "|    fps                | 871       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.353    |\n",
      "|    explained_variance | -5.96e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 0.0627    |\n",
      "|    value_loss         | 0.0305    |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10_000)\n",
    "\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    vec_env.render()\n",
    "    # VecEnv resets automatically\n",
    "    # if done:\n",
    "    #   obs = vec_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3\n",
    "from stable_baselines3.dqn import DQN\n",
    "from gym import spaces\n",
    "\n",
    "# Define the environment with five dimensions and three actions (buy, hold, and sell)\n",
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(5,))\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "    def _step(self, action):\n",
    "        # Perform the selected action and observe the reward and next state\n",
    "        reward = 0\n",
    "        next_state = None\n",
    "        done = False\n",
    "\n",
    "        # Update the environment and return the reward, next state, and done flag\n",
    "        return next_state, reward, done\n",
    "\n",
    "# Create the stock trading environment\n",
    "env = StockTradingEnv()\n",
    "\n",
    "# Create the DQN reinforcement learning model\n",
    "model = DQN(env, network='mlp')\n",
    "\n",
    "# Train the model using the DQN reinforcement learning algorithm\n",
    "model.learn(total_timesteps=1000, progress_bar=True)\n",
    "\n",
    "# Use the trained model to make predictions on new stock data\n",
    "predictions = model.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = obs.reshape((-1,) + model.observation_space.shape)\n",
    "observation = obs_as_tensor(observation, device)\n",
    "with th.no_grad():\n",
    "    q_values = model.q_net(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "model = DQN(\"MlpPolicy\", \"CartPole-v1\")\n",
    "env = model.get_env()\n",
    "\n",
    "obs = env.reset() #0번째 관측값\n",
    "with th.no_grad():\n",
    "     obs_tensor, _ = model.q_net.obs_to_tensor(obs)\n",
    "     q_values = model.q_net(obs_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv at 0x28e2b6a2640>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from stable_baselines3.dqn.policies import QNetwork, DQNPolicy\n",
    "\n",
    "# Override some methods of the class QNetwork used by the DQN model in order to set to a negative value the q-values of\n",
    "# some actions\n",
    "\n",
    "# Two possibile methods to override:\n",
    "# Override _predict ---> alter q-values only during predictions but not during training\n",
    "# Override forward ---> alter q-values also during training (Attention: here we are working with batches of q-values)\n",
    "\n",
    "class QNetwork_modified(QNetwork):\n",
    "    \n",
    "    def forward(self, obs: th.Tensor) -> th.Tensor:\n",
    "        \"\"\"\n",
    "        Predict the q-values.\n",
    "        :param obs: Observation\n",
    "        :return: The estimated Q-Value for each action.\n",
    "        \"\"\"\n",
    "        # Compute the q-values using the QNetwork\n",
    "        q_values = self.q_net(self.extract_features(obs))\n",
    "        # For each observation in the training batch:\n",
    "        for i in range(obs.shape[0]):\n",
    "            # Here you can alter q_values[i]\n",
    "\n",
    "        \n",
    "        return q_values\n",
    "\n",
    "    \n",
    "# Override the make_q_net method of the DQN policy used by the DQN model to make it use the new DQN network\n",
    "\n",
    "class DQNPolicy_modified(DQNPolicy):\n",
    "    def make_q_net(self) -> DQNPolicy:\n",
    "        # Make sure we always have separate networks for features extractors etc\n",
    "        net_args = self._update_features_extractor(self.net_args, features_extractor=None)\n",
    "        return QNetwork_modified(**net_args).to(self.device)\n",
    "\n",
    "\n",
    "\n",
    "model = DQN(DQNPolicy_modified, env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = [0.1, 0.2, 0.3]\n",
    "q_values = model.predict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "# Instantiate the agent\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "# Train the agent and display a progress bar\n",
    "model.learn(total_timesteps=int(2e5), progress_bar=True)\n",
    "# Save the agent\n",
    "model.save(\"dqn_lunar\")\n",
    "del model  # delete trained model to demonstrate loading\n",
    "\n",
    "# Load the trained agent\n",
    "# NOTE: if you have loading issue, you can pass `print_system_info=True`\n",
    "# to compare the system on which the model was trained vs the current one\n",
    "# model = DQN.load(\"dqn_lunar\", env=env, print_system_info=True)\n",
    "model = DQN.load(\"dqn_lunar\", env=env)\n",
    "\n",
    "# Evaluate the agent\n",
    "# NOTE: If you use wrappers with your environment that modify rewards,\n",
    "#       this will be reflected here. To evaluate with original rewards,\n",
    "#       wrap environment in a \"Monitor\" wrapper before other wrappers.\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "\n",
    "# Enjoy trained agent\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0 (default, Nov  6 2019, 16:00:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
